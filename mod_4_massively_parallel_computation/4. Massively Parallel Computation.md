## MPC Model
### Model definition
In the MPC model, every machine can communicate directly with every other, i.e., run on a complete graph.
The limit is on the machine memory, which is typically much smaller than the input. The input itself is arbitrarily distributed among the machines.
The total message size sent/received by a machine is limited by its memory.
For all models, the goal is to minimize the number of communication rounds.
### Parameters
The parameters of this model are:
- $N$: input size
- $S$: machine memory, measured in words

Where typically, we have $S=N^c$, with constant $c\lt1$, i.e., $S$ is polynomially smaller than $N$.
The input is arbitrarily distributed among the $M$ machines, so we need:
$$M\ge \dfrac NS$$
to be able to store the whole input.
The total size of the messages entering or leaving a machine are $\le S$.
The output can be stored in a single machine or distributed among multiple machines.
- - -
## Summing numbers
### Problem definition
The input is a list of $N$ numbers, and the output is the sum of these numbers.
Let's assume that $S=M=\Theta(\sqrt N)$.
### Algorithm
We can solve the problem in $2$ rounds:
- each machine computes the sum of its numbers and sends it to machine $1$
- machine $1$ computes and outputs the sum of the numbers received.

![[mpc-summing-numbers.png]]
### Complexity
The space and bandwidth bounds are satisfied:
- each machine sends only a single word
- the total size of the messages received by machine $1$ is $\le M=S$.
- - -
## Sorting
### Problem definition
The input is a list of $N$ numbers, and the output is a list of all $N$ pairs $(x,r)$, where:
- $x$ is an element of the list
- $r$ is the rank of $x$ in the sorted list

```ad-Esempio
[...]
```
Both input and output are spread among multiple machines.
The space available per machine is $S=N^\epsilon$, with $\epsilon\in(0,1)$.
The machines available are $M=\Theta\left(\dfrac NS\right)=\Theta(N^{1-\epsilon})$.
The goal is to sort in $O(1)$ rounds.
Our algorithm is a variant of QuickSort for MPC.
### Algorithm
Each machine samples each of its elements with some probability $p$, and sends its samples to machine $1$. In the next round, machine $1$ sorts the samples received from all machines, and sends back the sorted list to all of them.
![[]]
$S_i$ is the set of elements between sample $i-1$ and $i$ in sorted order.
Each machine counts the number of its elements in $S_i$, for each $i$, and sends these counts back to machine $1$.
From these counts, machine $1$ computes $|S_i|$ for each $i$.
From the sizes $|S_i|$, machine $1$ computes the rank of the samples and sends them to all machines.
Now what's left is to compute the ranks of non-sampled elements: each machine now knows the rank of each sample, hence if a machine can find the local rank in $S_i$ of an element $x$, it can obtain the global rank of $x$.
This means that the $S_i$ sets can be sorted recursively.
The problem is that $S_i$ may be too large to fit in one machine, so the solution is to assign a number of machines proportional to $|S_i|$.
Machine $1$ can use the sizes $|S_1|,|S_2|,\ldots$ to partition $[M]$ into sub-intervals where each sub-interval $[l_i,u_i]$ contains the indices of machines that should sort $S_i$.
```ad-Esempio

```

Each machine then does the following for each of its elements $x$:
- let $S_i\ni x$
- send $x$ to a random machine in $\[l_i,u_i]$.

Together, machines in $[l_i,u_i]$ recursively sort $S_i$.
The recursion stops when $|S_i|\le S$, a single machine then sorts $S_i$.
### Bounding the size received by each machine
Recall that the maximum size allowed is $S=N^\epsilon$.
The expected number of samples received by machine $1$ is $\le Np$. If we pick $p=\dfrac{n^{\epsilon/2}}{2N}$, from the Chernoff bound, w.h.p., the actual size received is at most:
$$2Np\le N^{\epsilon/2}\ll N^\epsilon=S$$
So, w.h.p., machine $1$ receives at most $1+N^{\epsilon/2}=O(N^{\epsilon/2})$ counts from each machine, which in total is:
$$O(MN^{\epsilon/2})=O(N^{1-\epsilon/2})=O(N)$$
This is too much in one round. We can solve this by spreading the data transfer over $O\left(\dfrac 1\epsilon\right)$ rounds using a converge-cast tree.
The expected numbers of pairs $(l_i,u_i)$ received are $Np+1$.
Using again Chernoff, we can prove that, w.h.p., the actual number is at most $2(Np+1)\ll S$.
For some $x$, $|S_i|=\Theta(x(u_i-l_i))$ for all $i$, proportionally.
The elements of $S_i$ are randomly assigned to machines in $[l_i,u_i]$.
Thus, each machine in $[l_i,u_i]$ receives $\dfrac{|S_i|}{u_i-l_i}=\Theta(x)$ elements in expectation.
To bound $x$, we do:
$$N\ge\sum_i|S_i|=\Theta\left(\sum_i x(u_i-l_i)\right)=\Theta(xM)\implies x=O\left(\dfrac NM\right)$$
W.h.p., each machine receives $O\left(\dfrac NM\right)$ elements.
We assumed $M=\dfrac{CN}S$ for a constant $C$, so for sufficiently big $C$, no machine receives more than $S$ elements w.h.p.
### Bounding size sent by each machine
Machine $1$ sends sorted samples to all machines. In expectation, the total size sent is $NpM$, but w.h.p., the actual size sent is at most:
$$2NpM=2N\cdot\dfrac{N^{\epsilon/2}}{2N}\cdot CN^{1-\epsilon}=O(N^{1-\epsilon/2})=O(N)$$
This is too much in a single round as we only allow $S=N^{\epsilon}$.
The solution, as before, is to spread the data transfer over $O\left(\dfrac1\epsilon\right)$ rounds using a broadcast tree: w.h.p., the size of the message sent per round per machine is less than $S$.
For the second part, we use the same approach.
The first part is dominated by $O\left(\dfrac1\epsilon\right)$ rounds for converge-cast and broadcast tree.
The second part has $O\left(\dfrac1\epsilon\right)$ rounds per recursion level. We will show that w.h.p., each $|S_i|=O(N^{1-\epsilon/3})$. Hence, the problem size is reduced by $\Omega(N^{\epsilon/3})$ per level.
This implies that the total number of levels is $O\left(\dfrac1\epsilon\right)$.
And so, w.h.p., the total number of rounds for the algorithm are:
$$O\left(\dfrac1\epsilon\right)\cdot O\left(\dfrac1\epsilon\right)=O(1)$$
Let $S_i$ the elements between sample $i-1$ and $i$ in sorted order.
$$\begin{align}
P[|S_i|\ge k]&=\overbrace{(1-p)^k}^{k\text{ unsampled in a row}}\\
&=\left(1-\dfrac{N^{\epsilon/2}}{2N}\right)^k\\
&=\left(1-\dfrac1{2N^{1-\epsilon/2}}\right)^k\\
&\le\exp\left(-\dfrac{k}{2N^{1-\epsilon/2}}\right)
\end{align}$$
Where in the last step we used $1+x\le e^x$.
With $k=D\cdot2N^{1-\epsilon/2}\ln(N)$ for constant $D$:
$$P[|S_i|\ge k]\le e^{-D\ln(N)}=N^{-D}$$
In conclusion, w.h.p., we have:
$$|S_i|=O(N^{1-\epsilon/2}\ln(N))=O(N^{1-\epsilon/3})$$
- - -
## Minimum Spanning Tree (MST)
### Problem definition
Let $G=(V,E)$ be a connected, edge-weighted, undirected graph.
Let $\text{MST}(E)$ denote an MST of $E$, same as MST of $G$.
We also define $n=|V|$ and $m=|E|$.
The input is a list of edges with weights, so $N=m$.
The output is the $\text{MST}(E)$.
The memory for each machine is $S=n^{1+\epsilon}$, for a constant $\epsilon\gt0$.
$M=\Theta\left(\dfrac NS\right)=\Theta\left(\dfrac{m}{n^{1-\epsilon}}\right)$, assuming $m=\Omega(n^{1+\epsilon})$
The goal is to compute $\text{MST}(E)$ in $O(1)$ rounds.
### Minimum Spanning Forest
A Minimum spanning forest of a graph $G=(V,E)$ is denoted by $\text{MSF}(E)$, and has a minimum spanning tree for each connected component.
```ad-Esempio

```
For simplicity, assume that any graph we consider has a unique MSF.
Let $E_1,\ldots,E_k$ be a partition of $E$.
Let $M_i=\text{MFS}(E_i)$ for $i=1,\ldots,k$.
Then:
$$\text{MFS}(E)=\text{MFS}\left(\bigcup_{i=1}^k E(M_i)\right)$$
```ad-Esempio
$M_1$ and $M_2$ are solid edges. 

$\text{MFS}(E)$ in yellow is contained in $E(M_1)\cup E(M_2)$:

```
### Shuffle & Filter algorithm
The shuffle algorithm for edge set $E'$ picks $k=\dfrac{2|E'|}{n^{1+\epsilon}}$ active machines and distributes the edges of $E'$ randomly among these.
The filter algorithm, for each active machine, computes a minimum spanning forest of assigned edge set.
Overall, the MST algorithm with initial $E'=E$:
- run shuffle algorithm for edge set $E'$
- run filter algorithm, let $M_1,\ldots,M_k$ be its output
- if $k=1$, then the output is $M_1$ and terminates
- otherwise, recurses on edge set $\displaystyle\bigcup_{i=1}^kE(M_i)$.

By the MSF property, $\text{MSF}(E')=\text{MSF}(E)$.
Hence, if the algorithm terminates, its output is:
$$M_1=\text{MSF}(E')=\text{MSF}(E)=\text{MST}(G)$$
### Proving number of steps
Recall that $k=\dfrac{}{}$ is the number of active machines
Let $k_1,k_2,\ldots$ be the sequence of $k$-values in the recursion steps.
Let $E_1',E_2',\ldots$ be the sequence of $E'$ sets in the recursion
For $j\gt1$, we have $E_j'$ as the union of $k_{j-1}$ forests, so:
$$|E_j'|\le k_{j-1}(n-1)$$
Hence:
$$k_j=\dfrac{2|E_j'|}{n^{1+\epsilon}}\le\dfrac{2k_{j-1}(n-1)}{n^{1+\epsilon}}\lt\dfrac{2}{n^\epsilon}k_{j-1}$$
Also:
$$k_1=\dfrac{2|E|}{n^{1+\epsilon}}\le\dfrac{2n^2}{n^{1+\epsilon}}=2n^{1-\epsilon}$$
Then:
$$k_j\lt2n^{1-\epsilon}\left(\dfrac2{n^\epsilon}\right)^{j-1}=n^{1-\epsilon j}2^j$$
Thus, the algorithm terminates after $O\left(\dfrac1\epsilon\right)=O(1)$ rounds.
### Bounding space and number of machines
In each round, there's $k=\dfrac{|E'|}{n^{1+\epsilon}}=\dfrac{2|E'|}S$ active machines.
The expected number of edges assigned to each such machine is:
$$\dfrac{|E'|}k=\dfrac{|E'|}{2|E'|/S}=\dfrac12|S|$$
From the Chernoff bound, we can prove that, w.h.p., the actual number of edges assigned is at most twice its expectation.
An active machine doesn't send more edges than it can store, so w.h.p., no machine exceeds $S$ space.
The number of machines used are then $O\left(\dfrac{|E|}S\right)=O\left(\dfrac NS\right)$.
### Proving the union
Let $E'=\displaystyle\bigcup_{i=1}^kE(M_i)$
Will show that for any $e=(u,v)\in\overbrace{E\smallsetminus E'}^{\text{dashed edges}}$, it's true that $e\notin\text{MSF}(E)$.
For some $i$, we have $e\in E_i\smallsetminus E(M_i)$.
This implies that $E(M_i)\cup\{e\}$ has a cycle $C$ containing $e$.
![[]]
$e$ has maximal weight on $C$:
- otherwise, let $e'\in E(C)$ with $w(e')\gt w(e)$
- let $M_i'=(M_i\smallsetminus\{e'\})\cup\{e\}$
- $M_i'$ is an MSF of $E_i$ of weight less than $M_i$, a contradiction

Assume for contradiction that $e\in\text{MSF}(E)$
![[]]
Removing $e$ from $\text{MSF}(E)$ splits a tree into two trees: $T_u$ containing $u$ and $T_v$ containing $v$.
$C\smallsetminus\{e\}$ is a path $P_{uv}$ in $G$ from $u$ to $v$.
Some edge $e_{uv}\in P$ connects $T_u$ with $T_v$.
By the property of $C$, reconnecting with $e_{uv}$ does not increase weight.
This contradicts the uniqueness of $\text{MSF}(E)$.
- - -