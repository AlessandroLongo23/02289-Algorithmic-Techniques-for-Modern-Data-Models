## Nearest Neighbor
### Definitions
```ad-Definizione
title: Nearest Neighbor
Given a set of points $P$ in a metric space, build a data structure which given a query point $x$, returns the point in $P$ closest to $x$.
```
```ad-Definizione
title: Metric
Distance function $d$ is a metric if:
- $d(x,y)\ge0$
- $d(x,y)=0\iff x=y$
- $d(x,y)=d(y,x)$
- $d(x,y)\le d(x,z)+d(z,y)$
```
### 1D
![[NN 1D.png]]
- - -
## Approximate Near Neighbors
$\texttt{ApproximateNearNeighbor}(x)$ returns a point such that:
$$d(x,y)\le c\cdot\min_{z\in P}d(x,z)$$
$c\texttt{-Approximate }r\texttt{-Near Neighbor}(x)$ returns, if there exists, a point $z$ in $P$ such that:
$$d(x,z)\le r$$ then return a point $y$ such that $d(x,y)\le c\cdot r$. If no such point $z$ exists, return $\texttt{Fail}$.
Randomized version: return such an $y$ with probability $\delta$.
![[Approximate near neighbor.png]]
### Locality Sensitive Hashing
A family of hash functions $H$ is $(r,cr,p_1,p_2)$-sensitive with $p_1>p_2$ and $c>1$ if:
- $d(x,y)\le r\implies P[h(x)=h(y)]\ge p_1$
- $d(x,y)\ge r\implies P[h(x)=h(y)]\le p_2$

for $h$ chosen randomly from $H$.
![[locality sensitive hashing.png]]
### Hamming distance
$P$ set of $n$ bit strings each of length $d$.
```ad-Definizione
title: Hamming distance
The number of bits where $x$ and $y$ differ:
$$d(x,y)=|\{i:x_i\ne y_i\}|$$
```
```ad-Esempio
title: Example
- $x=\textcolor{red}{10}1001\textcolor{red}{0}0$
- $y=\textcolor{red}{01}1001\textcolor{red}{1}0$

Hamming distance is $3$.
```
```ad-Definizione
title: Hash function
Chose $i\in\{1,\ldots, d\}$ uniformly at random and set $h(x)=x_i$.
What's the probability that $h(x)=h(y)$?
- $d(x,y)\le r\implies P[h(x)=h(y)]\ge1-\dfrac rd$
- $d(x,y)\ge cr\implies P[h(x)=h(y)]\le1-\dfrac{cr}d$
```
## LSH with Hamming Distance
### Solution 1
Pick random index $i$ uniformly at random. Let $h(x)=x_i$:
Bucket: strings with same hash value $h(x)$.
$\texttt{Insert}(x)$: insert $x$ in the list $A[h(x)]$
$\texttt{NearNeighbor}(x)$: compute Hamming distance form $x$ to all bitstring in $A[h(x)]$ until find one that is at most $cr$ away. If no such string is found, return $\texttt{Fail}$.
![[LSH Hamming 1.png]]
### Solution 2
Pick $k$ random indexes uniformly and independently at random with replacement
$$g(x)=x_{i_1}x_{i_2}\ldots x_{i_k}$$
```ad-Esempio
title: Example
$k=3$ and $g(x)=x_2x_3x_6$
- $x=1\textcolor{blue}{01}00\textcolor{blue}{1}00\implies g(x)=011$
- $y=1\textcolor{blue}{11}00\textcolor{blue}{1}10\implies g(y)=111$

Porbability that $g(x)=g(y)$:
- $d(x,y)\le r\implies P[g(x)=g(y)]\ge\left(1-\dfrac rd\right)^k$
- $d(x,y)\ge cr\implies P[g(x)=g(y)]\le\left(1-\dfrac{cr}d\right)^k$
```
![[LSH hamming 2.png]]
Buckets: strings with same hash value $g(x)$.

if $g(x)=x_2x_4x_7$:
- $a=0011101\implies g(a)=011$
- $b=0101001\implies g(b)=111$
- $c=0010010\implies g(c)=000$
- $d=0110011\implies g(d)=101$
- $e=1011101\implies g(e)=011$
- $f=1101101\implies g(f)=111$

![[buckets.png]]
Save buckets in a hash table $T$ with hash function $h_T$:
- $h_T(011_2)=1$
- $h_T(111_2)=6$
- $h_T(000_2)=9$
- $h_T(101_2)=1$

![[bucket hash table.png]]
The $\texttt{Insert}(x)$ inserts $x$ in the list of $g(x)$ in $T$.
The $\texttt{NearNeighbor}(x)$ compute Hamming distance from $x$ to all bitstrings in $g(x)$ until find one that is that is at most $cr$ away. If no such string found return $\texttt{Fail}$.
When we increase $k$:
- **far away strings**: the probability that a far away string hashes to the same bucket as $x$ decreases
- **close strings**: the probability that a close string hashes to the same as $x$ also decreases

![[changing k.png]]
The expected number of far away strings that hashes to the same bucket as $x$ is:
$$F=\{y:d(x,y)>cr\}$$
For $y\in F$ we want $P[g(y)=g(x)]\le\dfrac1n$. 
Set $k=\dfrac{\log_2(n)}{\log_2(1/p_2)}$
$$X_y=\begin{cases}
1&y\text{ collides with }x\\
0&\text{otherwise}
\end{cases}$$
Far away strings colliding with $x$:
$$X=\sum_{y\in F}X_y$$
$$\mathbb E[X]=\sum_{y\in F}\mathbb E[X_y]=\sum_{y\in F}\dfrac1n\le 1$$
From the Markov's inequality, we get:
$$P[X>6]<\dfrac{\mathbb E[X]}6\le\dfrac16$$
[...]
### Solution 3: Amplification
Construct $L$ hash tables $T_j$, each with its own independently chosen hash function $h_j$ and its own independently chosen locality sensitive hash function $g_j$.
- $\texttt{Insert}(x)$: for all $1\le j\le L$ insert $x$ in the list $g_j(x)$ in $T_j$
- $\text{Query}(x)$: for all $1\le j\le L$ check each element in bucket $g_j(x)$ in $T_j$. Return the one closest to $x$ if it is at most $cr$ away. Otherwise return $\texttt{Fail}$.

![[Amplification.png]]
Let $k=\dfrac{log_2(n)}{\log_2(1/p_2)}$, and $\rho=\dfrac{\log_2(1/p_1)}{\log_2(1/p_2)}$, and $L=\lceil 2n^\rho\rceil$, where:
- $p_1=1-\dfrac rd$
- $p_2=1-\dfrac{cr}d$

If there exists a string $z^*$ in $P$ with $d(x,z^*)\le r$, then with probability at least $\dfrac56$ we will return some $z$ in $P$ for which $d(x,y)\le r$.
The probability that $z^*$ collides with $x$ is:
$$\begin{align}
P[\exists i:g_i(x)&=g_i(z^*)]=1-P[g_i(x)\ne g_i(z^*)\text{ for all }i]=\\
&=1-\prod_{i=1}^LP[g_i(x)\ne g_i(z^*)]=\\
&=1-\prod_{i=1}^L(1-P[g_i(x)=g_i(z^*)])\ge\\
&\ge1-\prod_{i=1}^L(1-p_1)^k=\\
&=1-(1-p_1^k)^L\ge\\
&\ge1-e^{-Lp_1^k}\ge\\
&\ge1-\dfrac1{e^2}\ge\\
&\ge1-\dfrac16=\dfrac56
\end{align}$$
![[solution 3 claim.png]]
### Fast query time
Check at most $6L+1$ strings and return $\texttt{Fail}$ if no close string is found, otherwise return the closest string found.
```ad-Teorema
title: Theorem
If there exists a string $z^*$ in $P$ with $d(x,z^*)\le r$, then with probability at least $\dfrac23$ we will return some $y$ in $P$ for which $d(x,y)\le cr$.
```
The idea to prove this is to show that with probability at least $\dfrac56$ there are at most $6L$ far away strings that collides with $x$, and we already showed that the probability that $z^*$ is in the same bucket as $x$ in at least one of the $L$ hash tables is at least $\dfrac56$.
![[Fast query time.png]]
So:
- insert time is $O(kL)$
- expected query time is $O(L(k+d))$, in fact we have $O(L)$ checks, each that takes $O(k+d)$ time

- - -
## Other distances
### Jaccard distance and Min Hash
The Jaccard similarity is defined as:
```ad-Definizione
title: Jaccard similarity
$$\text{Jsim}(A,B)=\dfrac{|A\cap B|}{|A\cup B|}$$
```
From the similarity, we can then define the Jaccard distance as:
```ad-Definizione
title: Jaccard distance
$$1-\text{Jsim}(A,B)$$
```
### Angular distance and Sim Hash
Suppose we have a collection of vectors.
Distance between two vectors is the angular distance between them:
$$\text{dist}(u,v)=\dfrac{\angle(u,v)}\pi$$
Hash function: Sim Hash
Random projection: take a random vector $r$ and set $h_r(u)=\text{sign}(r\cdot u)$
![[Sim hash.png]]
[...]
We can show that $P[h(u)=h(v)]=1-\dfrac{\angle(u,v)}\pi$
- - -
