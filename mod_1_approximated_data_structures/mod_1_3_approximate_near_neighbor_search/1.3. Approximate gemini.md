# Approximate Near Neighbor Search
## 1. Nearest Neighbor
### Definitions
```ad-Definizione
title: Nearest Neighbor
Given a set of points $P$ in a metric space, build a data structure which, given a query point $x$, returns the point in $P$ closest to $x$.
```
```ad-Definizione
title: Metric
A distance function $d$ is a metric if it satisfies the following properties for all $x, y, z$:
- $d(x,y) \ge 0$ (Non-negativity)
- $d(x,y) = 0 \iff x=y$ (Identity)
- $d(x,y) = d(y,x)$ (Symmetry)
- $d(x,y) \le d(x,z) + d(z,y)$ (Triangle Inequality)
```
- - -
## 2. Approximate Near Neighbors (ANN)
The exact Nearest Neighbor problem can be computationally expensive in high dimensions. We often relax the problem to find an "approximate" neighbor.
**The Function `ApproximateNearNeighbor(x)`**
Returns a point $y \in P$ such that:
$$d(x,y) \le c \cdot \min_{z\in P}d(x,z)$$
where $c$ is the approximation factor.
**The Decision Problem: $c$-Approximate $r$-Near Neighbor**
Given a radius $r$ and a query $x$:
1.  **If** there exists a point $z \in P$ such that $d(x,z) \le r$, return a point $y$ such that $d(x,y) \le c \cdot r$.
2.  **If** no such point $z$ exists, return `Fail`.

**Randomized version:** Return such a point $y$ with probability $\delta$.
![[Approximate near neighbor.png]]
- - -
## 3. Locality Sensitive Hashing (LSH)
LSH is a technique to solve the ANN problem. The goal is to hash similar items into the same bucket with high probability, and dissimilar items into different buckets.
### Definition
A family of hash functions $H$ is $(r, cr, p_1, p_2)$-sensitive with $c > 1$ and $p_1 > p_2$ if, for a hash function $h$ chosen randomly from $H$:
- **Close points collide:** $d(x,y) \le r \implies P[h(x)=h(y)] \ge p_1$
- **Far points don't collide:** $d(x,y) \ge cr \implies P[h(x)=h(y)] \le p_2$

![[locality sensitive hashing.png]]
- - -
## 4. LSH with Hamming Distance
### Context
Let $P$ be a set of $n$ bit strings, each of length $d$.
```ad-Definizione
title: Hamming Distance
The number of bits where $x$ and $y$ differ:
$$d(x,y) = |\{i : x_i \ne y_i\}|$$
```
```ad-Esempio
title: Example
- $x=\textcolor{red}{10}1001\textcolor{red}{0}0$
- $y=\textcolor{red}{01}1001\textcolor{red}{1}0$
- Hamming distance is $3$.
```
### Basic Hash Function
Choose an index $i \in \{1, \ldots, d\}$ uniformly at random and set $h(x) = x_i$.
**Probabilities:**
The probability that $h(x) = h(y)$ is the probability that they match at the chosen index (which is $1 - \dfrac{d(x,y)}{d}$).
- If $d(x,y) \le r \implies P[h(x)=h(y)] \ge 1 - \dfrac{r}{d}$ (This is $p_1$)
- If $d(x,y) \ge cr \implies P[h(x)=h(y)] \le 1 - \dfrac{cr}{d}$ (This is $p_2$).
- - -
## 5. Algorithms for LSH with Hamming
### Solution 1: Single Index (Too Weak)
- **Algorithm:** Pick a random index $i$. Insert $x$ into bucket $h(x)$.
- **Query:** Check all items in the bucket.
- **Problem:** $p_1$ and $p_2$ are too close; too many false positives or negatives.
### Solution 2: Concatenation (AND Construction)
Pick $k$ indices uniformly and independently at random with replacement. Define the hash function as the concatenation of bits at these indices:
$$g(x) = x_{i_1}x_{i_2}\ldots x_{i_k}$$

**Probabilities:**
Since we need *all* $k$ positions to match:
- $d(x,y) \le r \implies P[g(x)=g(y)] \ge (1 - \frac{r}{d})^k$
- $d(x,y) \ge cr \implies P[g(x)=g(y)] \le (1 - \frac{cr}{d})^k$

**Effect:** This widens the gap between high and low probabilities, but it drastically lowers the total probability of collision for *both* close and far points.

**Tuning $k$:**
We want to minimize collisions with "far away" strings.
Let $F = \{y : d(x,y) > cr\}$ (the set of far strings). We want $P[g(y)=g(x)] \le \frac{1}{n}$ for $y \in F$.
We set:
$$k = \frac{\log_2(n)}{\log_2(1/p_2)}$$

**Expected Collisions:**
Let $X_y = 1$ if $y$ collides with $x$, $0$ otherwise. The total number of far string collisions is $X = \sum_{y \in F} X_y$.
$$\mathbb{E}[X] = \sum_{y \in F} \mathbb{E}[X_y] = \sum_{y \in F} \frac{1}{n} \le 1$$
By Markov's Inequality ($P[X \ge a] \le \frac{\mathbb{E}[X]}{a}$):
$$P[X > 6] < \frac{1}{6}$$
This means the bucket is unlikely to be crowded with far strings, but we might miss the *close* string because the collision probability dropped too low.

### Solution 3: Amplification (OR Construction)
To fix the low collision probability of Solution 2, we use **$L$ independent hash tables**.

**Algorithm:**
1.  Construct $L$ hash tables $T_1, \ldots, T_L$.
2.  Each table $T_j$ uses an independent composite hash function $g_j(x)$ (from Solution 2).
3.  **Insert(x):** For all $1 \le j \le L$, insert $x$ into bucket $g_j(x)$ in table $T_j$.
4.  **Query(x):** For all $1 \le j \le L$, retrieve items from bucket $g_j(x)$. Check distances. Return the closest point if it is $\le cr$. Else return `Fail`.

**Parameters:**
- $k = \frac{\log_2(n)}{\log_2(1/p_2)}$
- Let $\rho = \frac{\log_2(1/p_1)}{\log_2(1/p_2)}$
- $L = \lceil 2n^\rho \rceil$

**Probability of Success:**
If there exists a close string $z^*$ (where $d(x, z^*) \le r$), what is the probability we find it in *at least one* table?
$$
\begin{align}
P[\exists j : g_j(x) = g_j(z^*)] &= 1 - P[\forall j, g_j(x) \ne g_j(z^*)] \\
&= 1 - \prod_{j=1}^L (1 - P[g_j(x)=g_j(z^*)]) \\
&\ge 1 - \prod_{j=1}^L (1 - p_1^k) \\
&= 1 - (1 - p_1^k)^L \\
&\ge 1 - e^{-L p_1^k} \quad \text{(using } 1-x \le e^{-x}) \\
&\ge 1 - \frac{1}{e^2} > \frac{5}{6}
\end{align}
$$
(Note: $L \cdot p_1^k \approx 2$ based on the choice of $L$ and $k$).

**Fast Query Time:**
To ensure speed, we check at most $6L$ strings total. If we haven't found a match after checking $6L$ candidates, we stop.

> **Theorem:** If there exists a string $z^*$ with $d(x,z^*) \le r$, then with probability at least $\frac{2}{3}$ we will return some $y$ where $d(x,y) \le cr$.

**Complexity:**
- **Insert Time:** $O(kL)$
- **Query Time:** Expected $O(L(k+d))$

---

## 6. Other Distances
We can adapt LSH to other metrics by changing the hash function.
### Jaccard Distance (Sets)
- **Similarity:** $\text{Jsim}(A,B) = \frac{|A \cap B|}{|A \cup B|}$
- **Distance:** $1 - \text{Jsim}(A,B)$
- **LSH Technique:** **MinHash**.
    - Pick a random permutation of the universe of elements.
    - $h(A) = \min(\pi(A))$ (the first element of A in the permutation).
    - $P[h(A) = h(B)] = \text{Jsim}(A,B)$.

### Angular Distance (Vectors)
- **Distance:** The angle between vectors $u, v$, normalized by $\pi$:
    $$\text{dist}(u,v) = \frac{\angle(u,v)}{\pi}$$
- **LSH Technique:** **SimHash** (Random Hyperplane).
    - Pick a random vector $r$ (unit sphere).
    - $h_r(u) = \text{sign}(r \cdot u)$.
    - $P[h(u) = h(v)] = 1 - \frac{\angle(u,v)}{\pi}$.