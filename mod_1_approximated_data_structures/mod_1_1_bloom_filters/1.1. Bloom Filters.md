### Independent Random Variables
[...]
- - -
## Hash functions for Bloom filters
A hash function is a mapping $h:U\to M$ from a universe $U$ of size $u$ to a set $M=\{1,\ldots,m\}$. Typically $m\ll u$.
For the analysis of Bloom filters, we need certain properties of $k$ hash functions:
$$h_1,\ldots,h_k$$
### Uniform hashing
Each $h_i$ maps each element $x\in U$ to $M$ uniformly at random:
$$P[h_i(x)=j]=\dfrac1m$$
for $j=1,\ldots,m$
### Independence
The $ku$ random variables $h_i(x)$ for $i=1,\ldots,k$ and $x\in U$ are independent.
```ad-Esempio
title: Example
For instance, for any $x,y\in U$:
$$P[h_1(x)=2,h_2(y)=4]=P[h_1(x)=2]\cdot P[h_2(y)=4]$$
```
## Problem definition
We are given a universe $U$ of size $u$ and a subset $X=\{x_1,\ldots,x_n\}$ of $U$ of size $n$.
We need to support two types of operations:
- inserting an element of $U\smallsetminus X$ into $X$
- answer a query of the form: "Is $x\in X$?" for any query element $X\in U$

## Bloom filter
### Definition
A Bloom filter for representing a set $X\subseteq U$ consists of:
- a bit array $M$ of length $m$ with indices $1,\ldots, m$
- $k$ hash functions $h_1,\ldots,h_k:U\to\{1,\ldots,m\}$

We assume that the hash functions have the properties stated earlier. 
### Representation
To represent $X$, the bits of $M$ are first initialized $0$. Then, for each $x\in X$, we compute the resulting index hash functions and we set $M[h_i(x)]$ to $1$ for all $i=1,\ldots,k$.
```
proc represent(M, X):
	for i in m:
		M[i] = 0
	
	for x in X:
		for i in k:
			M[h_i(x)] = 1
```
This is also how we insert any new element $x$.
```
proc insert(M, x):
	for i in k:
		M[h_i(x)] = 1
```
### Queries
To answer a query, i.e., to know if a certain $x$ is in $M$, we check the bits in the Bloom filter given by the $k$ hash functions when querying $x$ on them: if they're all $1$, we return $\texttt{True}$, and $\texttt{False}$ otherwise.
```
proc query(M, x):
	for i in k:
		if M[h_i(x)] = 0
			return False
	
	return True
```
![[Bloom filters true negatives.png]]
- $M[h_i(x)]=0$ for at least one $i$, since the answer is $\texttt{False}$
- no element of $X$ has this property, by construction

The conclusion is that, we must have $x\notin X$, i.e., the Bloom filter is always correct when it answers $\texttt{False}$ to a query for $x$.
In other words, the Bloom Filters has **no false negatives**.
If the bloom filter answers $\texttt{True}$, we can't be sure that $x\in X$: we could have $x\in X$ and still all bits $h_i(x)$ of $M$ set to $1$ by other elements of $X$.
![[Bloom filters false positives.png]]
Thus, the Bloom filter might be wrong when it answers $\texttt{True}$, i.e., it **can have false positives**.
### Bounding the chance of false positives
Suppose a $\rho$-fraction of the bits of $M$ are $0$. Call this event $\mathcal E$.
Given $x\notin X$, we want to bound the probability that the Bloom filter answers $\texttt{Yes}$ to $x$.
Random variables $h_1(x),\ldots,h_k(x)$ are:
- uniformly distributed in $\{1,\ldots,m\}$
- independent of $\mathcal E$, i.e., $x$ is not considered in the construction of $M$

This implies that for any $i$:
$$P[M[h_i(x)]=1]=1-P[M[h_i(x)]=0]=1-\rho$$
Using independence of $h_1(x),\ldots,h_k(x)$, the probability that the Bloom filter incorrectly answers $\texttt{Yes}$ for $x$ is thus:
$$P[M[h_1(x)]=1,\ldots,M[h_k(x)]=1]=(1-\rho)^k$$
### Parametrizing the failure probability
Our goal now is, given $m$ and $n$, to pick the number $k$ of hash functions to minimize the chance of false positives.
Let $p'$ be the probability that a specific bit $j$ of $M$ is $0$, i.e., that none of the $k$ hash functions $h_i$ map any of the $n$ elements of $X$ to that bit:
$$p'=\left(1-\dfrac1m\right)^{kn}$$
We have $E[\rho]=p'$.
It can be shown that $\rho$ is concentrated around its expectation, meaning that w.h.p.,
$$\rho\approx \mathbb E[\rho]=p'$$
```ad-Esempio
title: Example
If each bit has a $p'=50\%$ chance of being $0$, we expect the fraction $\rho$ of $0$-bits in $M$ to also be $\dfrac12$.
```
Thus, the probability of a false positive is:
$$(1-\rho)^k\approx(1-p')^k=\left(1-\left(1-\dfrac1m\right)^{kn}\right)^k$$
We simplify this with the approximation $1+x\approx e^x$ for $x$ close to $0$.
Setting $x=-\dfrac1m$, the approximate probability then becomes:
$$\left(1-\left(e^{-\frac1m}\right)^{kn}\right)^k=\left(1-e^{-\frac{kn}m}\right)^k=\text{exp}\left(k\ln\left(1-e^{-\frac{kn}m}\right)\right)$$
We want to pick $k$ to minimize this expression.
Standard calculus shows that this occurs for:
$$k_\min=\dfrac mn\ln(2)$$
for which we get a false positive rate $\epsilon$ of:
$$\begin{align}
\epsilon=f(k_\min)&=\exp\left(\left(\dfrac mn\ln(2)\right)\ln\left(1-e^{-(\frac mn\ln(2))\frac nm}\right)\right)=\\
&=\exp\left(\left(\dfrac mn\ln(2)\right)\ln\left(1-\dfrac12\right)\right)=\\
&=\exp\left(-\dfrac mn(\ln(2))^2\right)=\\
&=2^{-\frac mn\ln(2)}=\\
&=2^{-k_\min}=\left(\dfrac12\right)^{k_\min}
\end{align}$$
Equivalently, this is:
$$2^{-\frac mn\ln(2)}=(2^{\ln(2)})^{\frac mn}\approx 0.6185^\frac mn$$
Fix $k=k_\min=\dfrac mn\ln(2)$ to minimize the chance of false positives.
### Space requirement
To analyze space, take the base $2$ logarithm and isolate $m$:
$$\log_2(1/\epsilon)=\dfrac mn\ln(2)\implies m=\dfrac{n\log_2(1/\epsilon)}{\ln(2)}=n\log_2(e)\log_2(1/\epsilon)$$
Which is the number $m$ of bits stored.
- - -
## Better data structures
We will show that it's not possible to get a data structure requiring significantly less space than a bloom filter if we allow no false negatives and allow false positives for at most an $\epsilon$ fraction of elements of $U\smallsetminus X$: only minor improvements in space are possible.
### Lower space bound
Consider any such data structure and let $m$ be the number of bits it requires. Each instance $X$ gives rise to such an $m$-bit string and we say that $X$ is represented by this string.
Consider an $m$-bit string $M$, i.e., one instance of the data structure.
Let $Y(M)$ be the set of elements of $U$ that $M$ answers $\texttt{Yes}$ to.
For any $X$ represented by $M$, we must have $X\subseteq Y(M)$, i.e., no false negatives.
We allow at most a false positive rate of $\epsilon$ for $U\smallsetminus X$. Thus, $Y(M)$ contains at most $\epsilon(u-n)$ elements in addition to $X$.
![[Bloom filter lower space bound.png]]
It follows that $|Y(M)|\le n+\epsilon(u-n)$.
$M$ can thus not represent more than $\dbinom{n+\epsilon(u-n)}n$ subsets $X$ since they all need to be contained in $Y(M)$.
There are $2^m$ choices of $m$-length bit string $M$ and each represents at most $\dbinom{n+\epsilon(u-n)}n$ sets $X$.
Hence, the data structure cannot represent more sets than:
$$2^m\dbinom{n+\epsilon(u-n)}n$$
However, it needs to represent all of the $\dbinom un$ sets $X$, so:
$$2^m\dbinom{n+\epsilon(u-n)}n\ge\dbinom un$$
Taking the logarithm and assuming $n\ll \epsilon u$, we isolate $m$. 
```ad-Nota
title: Note
For $b\ll a$, it is true that:
$$\dbinom ab\approx\dfrac{a^b}{b!}$$
In fact:
$$\dbinom ab=\dfrac{a!}{b!(a-b)!}=\dfrac{a(a-1)\ldots(a-b+1)}{b!}\approx\dfrac{a^b}{b!}$$
```
```ad-Nota
title: Note
Similarly, $\dbinom un\approx\dfrac{u^n}{n!}$ since $n\ll \epsilon u\le u$.
```
With this two tricks, we can approximate:
$$\begin{align}
m&\ge\log_2\left(\dfrac{\dbinom un}{\dbinom{n+\epsilon(u-n)}n}\right)\approx\log_2\left(\dfrac{\dbinom un}{\dbinom{\epsilon u}n}\right)\approx\\
&\approx\log_2\left(\dfrac{\dfrac{u^n}{n!}}{\dfrac{(\epsilon u)^n}{n!}}\right)=\log_2((1/\epsilon)^n)=n\log_2(1/\epsilon)
\end{align}$$
We have shown a lower bound on $m$ of $n\log_2(1/\epsilon)$ bits.
Recall that the bloom filter requires $n\log_2(e)\log_2(1/\epsilon)$ bits of space.
We see that the space requirement of the Bloom filter is within a factor $\log_2(e)\approx1.44$ of the lower bound.
More complicated data structures with better space bound exist, for instance compressed Bloom filters.
Our analysis relied on hash functions with strong independence guarantees, but it's not known how to ensure such guarantees without using a lot of space (around $n\log(n)$ bits).
Fortunately, Bloom filters work well using much more practical hash functions with weaker guarantees.
- - -