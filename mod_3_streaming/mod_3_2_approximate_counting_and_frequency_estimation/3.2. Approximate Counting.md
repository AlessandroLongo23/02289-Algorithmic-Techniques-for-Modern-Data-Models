## Hash functions
For any positive integer $a$, we indicate the set of integers up to $a$ with:
$$[a]=\{1,2,\ldots,a\}$$
A hash function $h:[m]\to[n]$ is $2$-universal if:
$$P[h(x_1)=y_1\land h(x_2)=y_2]=\dfrac1{n^2}$$
for all distinct $x_1,x_2\in[m]$ and for all $y_1,y_2\in[n]$.
Useful properties of a $2$-universal hash function $h$ are:
- $h$ is uniform, i.e.:$$P[h(x)=y]=\dfrac 1n\quad\forall x\in[m],y\in[n]$$
- $h$ hashes any two distinct values $x_1,x_2$ independently:$$P[h(x_1)=y_1\land h(x_2)=y_2]=P[h(x_1)=y_1]\cdot P[h(x_2)=y_2]$$

### The distinct elements problem
Given a stream $\sigma=\langle a_1,\ldots a_m\rangle$ with each $a_i\in[n]$.
This defines a frequency vector $f=(f_1,\ldots,f_n)$
```ad-Esempio
title: Example
With $n=4$ and $m=10$:
$$\sigma=\langle 4,2,4,1,4,2,4,4,1,2\rangle$$
$$f=(2,3,0,5)$$
```
Let $d=|\{j|f_j\gt0\}|$ be the number of distinct elements.
In the example above, $d=|\{1,2,4\}|=3$.
The algorithm output is an $(\epsilon,\delta)$-estimate of $d$, which means that $\hat d$ should satisfy:
$$P\left[\left|\dfrac{\hat d}d-1\right|\gt\right]\le\delta$$
### Zeros of an integer
For an integer $p\ge 0$, we defines with $\text{zeros}(p)$ as the number of zeros that $p$ ends with in its binary representation.
```ad-Esempio
title: Example
- $\text{zeros}(2)=1$
- $\text{zeros}(3)=0$
- $\text{zeros}(16)=4$
- $\text{zeros}(24)=3$
```
We can also write $\text{zeros}(p)$ as:
$$\text{zeros}(p)=\max\{i|2^i\text{ divides }p\}$$
```ad-Esempio
title: Example
$\text{zeros}(24)=3$ since $2^3=8$ is the largest power of $2$ that divides $24$.
```
### The Tidemark Algorithm (AMS Algorithm)
Pseudo-code:
```
proc tidemark():
	choose a 2-universal hash function h
	z = 0
	
	for j in stream:
		z = max(z, zeros(h(j)))
	
	output 2^{z+1/2}
```
Thus, for the stream $\sigma=\langle a_1,\ldots,a_m\rangle$, the final value for $z$ is:
$$z=\max_{i\in[m]}\{\text{zeros}(h(a_i))\}$$
We now analyze how good an estimate to $d$ the algorithm obtains.
### Intuition
Every $d$-th value in $[n]$ ends with at least $\log_2(d)$ zeros.
```ad-Esempio
title: Example
```
Only few of these values have significantly more than $\log_2(d)$ zeros.
$d$ values are hashed to $[n]$ over the entire stream.
Since these values are hashed uniformly, $z$ should be close to $\log_2(d)$ at termination.
This gives output:
$$2^{z+1/2}\approx 2^{\log_2(d)+1/2}\approx2^{\log_2(d)}=d$$
We will now prove this more formally.
Consider a token $j\in[n]$ and any integer $r\ge 0$.
$X_{r,j}$ is an indicator variable for the event that $h(j)$ has at least $r$ zeros.
$$X_{r,j}=1\iff\text{zeros}(h(j))\ge r$$
Let random variable $Y_r$ count the number of such tokens:
$$Y_r=\sum_{j:f_j\gt0}X_{r,j}$$
```ad-Nota
title: Note
If token $j$ occurs, e.g., $f_j=10$ times in the stream, it only contributes with $0$ or $1$ to $Y_r$.
```
In the following, let $z_\text{out}$ be the value of $z$ at termination.
We have $z_\text{out}\ge r$ if and only if for at least one token $j$:
$$\text{zeros}(h(j))\ge r$$
Since $Y_r$ counts the number of such tokens:
$$Y_r\ge1\iff z_\text{out}\ge r$$
Equivalently:
$$Y_r=\iff z_\text{out}\le r-1$$
Since $X_{r,j}$ is an indicator variable:
$$\mathbb E[X_{r,j}]=P[X_{r,j}=1]=P[\text{zeros}(h(j))\ge r]$$
$h$ is $2$-universal, and so uniform. Thus only a $\dfrac1{2^r}$ fraction has $\text{zeros}(i)\ge r$.
And so, $h(x)$ has only a $\dfrac1{2^r}$ chance of hitting one such $i$.
This gives:
$$\mathbb E[X_{r,j}]=P[\text{zeros}(h(j))\ge r]=\dfrac1{2^r}$$
By linearity of expectation:
$$\mathbb E[Y_r]=\sum_{j:f_j\gt0}\mathbb E[X_{r,j}]=\dfrac d{2^r}$$
Let $\hat d=2^{z_\text{out}+1/2}$ be the estimate of $d$ by the algorithm.
We will bound the probability that it deviates too much from $d$:
$$P[\hat d\ge3d]\le\dfrac{\sqrt{2}}3\approx 0.47$$
and
$$P\left[\hat d\le\dfrac d3\right]\le\dfrac{\sqrt{2}}3\approx0.47$$
### Part 1
Let $a$ be the smallest integer with $2^{a+1/2}\ge 3d$
$a$ is the smallest $z_\text{out}$ giving output $\hat d\ge 3d$, so:
$$P[\hat d\ge3d]=P[2^{z_\text{out}+1/2}\ge3d]=P[z_\text{out}\ge a]=P[Y_a\ge1]$$
By Markov's inequality:
$$P[Y_a\ge 1]\le\dfrac{\mathbb E[Y_a]}1=\mathbb E[Y_a]=\dfrac d{2^a}$$
We then get:
$$P[\hat d\ge 3d]=P[Y_a\ge1]\le\dfrac d{2^a}\le\dfrac{2^{a+1/2}/3}{2^a}=\dfrac{\sqrt{2}}3$$
### Part 2
Let $b$ be the largest integer with $2^{b+1/2}\le\dfrac d3$
$b$ is the largest $z_\text{out}$ giving output $\hat d\le\dfrac d3$, so:
$$P\left[\hat d\le\dfrac d3\right]=P\left[2^{z_\text{out}+1/2}\le\dfrac d3\right]=P[z_\text{out}\le b]=P[Y_b=0]$$
Since $d\ge3\cdot2^{b+1/2}$, we get:
$$P\left[\hat d\le\dfrac d3\right]=P[Y_{b+1}=0]\le\dfrac {2^{b+1}}d\le\dfrac{2^{b+1}}{3\cdot2^{b+1/2}}=\dfrac{\sqrt{2}}3$$
To use Chebyshev's inequality, we need $\text{Var}[Y_r]$:
Since the $X_{r,j}$-variables are functions of hash values, which are $2$-universal hash functions, they too are $2$-independent. Thus, we can use linearity of variance:
$$\text{Var}[Y_r]=\text{Var}\left[\sum_{j:f_j\gt0}X_{r,j}\right]=\sum_{j:f_j\gt0}\text{Var}[X_{r,j}]$$
```ad-Teorema
title: Lemma
For any random variable $X$:
$$\text{Var}[X]\le\mathbb E[X^2]$$
```
```ad-Dimostrazione
title: Proof
[...]
```

Since $X_{r,j}$ is an indicator variable, $X_{r,j}^2=X_{r,j}$, and since $\mathbb E[X_{r,j}]=\dfrac1{2^r}$:
$$\begin{align}
\text{Var}[Y_r]&=\sum_{j:f_j\gt0}\text{Var}[X_{r,j}]\le\\
&\le\sum_{j:f_j\gt0}\mathbb E[X_{r,j}^2]\\
&=\sum_{j:f_j\gt0}\mathbb E[X_{r,j}]=\dfrac d{2^r}
\end{align}$$
We have shown that both $\mathbb E[Y_r]$ and $\text{Var}[Y_r]$ are $\dfrac d{2^r}$.
We have the following implication between events:
$$Y_r=0\implies|Y_r-\mathbb E[Y_r]|=|E[Y_r]|\ge\dfrac d{2^r}$$
Thus, the left-hand side is not more likely that the right-hand size:
$$P[Y_r=0]\le P\left[|Y_r-\mathbb E[Y_r]|\ge\dfrac d{2^r}\right]$$
From Chebyshev:
$$P[Y_r=0]\le P\left[|Y_r-\mathbb E[Y_r]|\ge\dfrac d{2^r}\right]\le\dfrac{\text{Var}[Y_r]}{(d/2^r)^2}\le\dfrac{d/2^r}{(d/2^r)^2}=\dfrac{2^r}d$$
- - -
## Approximate Counting
### Problem definition
The problem is to count the length $n$ of the stream seen so far using as few bits as possible.
It's trivial with $O(\log(m))$ bits. This is in fact optimal: we can do better if we only need an estimate of $m. 
### Morris counter
We will analyze Morris counter, which with slight modifications, can obtain an $(\epsilon,\delta)$-estimate using only $O(\log(\log(m)))$ bits, for constant $\epsilon$ and $\delta$
Instead, we show that its output is an unbiased estimator of $n$.
#### Space efficient version
```
proc morris_efficient(S):
	x = 0
	for x in S:
		with probability 2^{-x}:
			x++
	
	return 2^x-1
```
#### Space inefficient version
```
proc morris_inefficient(S):
	c = 1
	for x in S:
		with probability 1/c:
			c = 2 * c
	
	return c - 1
```
The algorithms give the same output, since in each iteration, $c=2^x$.
We focus on the second, since it's easier to analyze.
### Unbiased estimator
Let $C_i$ be $c$ after processing $i$ tokens, and $C_0=1$.
The output after $n$ tokens is $C_n-1$.
We need to show that $C_n-1$ is an unbiased estimator of $n$:
$$\mathbb E[C_n-1]=n$$
Let $Z_i$ indicate if $c$ doubles when processing token $i+1$:
$$Z_i=\begin{cases}1&C_{i+1}=2C_i\\0&C_{i+1}=C_1\end{cases}$$
Or, in other words:
$$C_{i+1}=C_i(1+Z_i)$$
When processing token $i+1$, the probability $\dfrac1c$ is $\dfrac1{C_i}$.
$$\mathbb E[Z_i|C_i]=P[Z_i=1|C_i]=\dfrac1{C_i}$$
From the law of total expectation, for any random variables $X$ and $Y$:
$$\mathbb E[X]=\mathbb E[\mathbb E[X|Y]]$$
Applying this with $X=C_{i+1}$ and $Y=C_i$, we get:
$$\begin{align}
\mathbb E[C_{i+1}]&=\mathbb E[\mathbb E[C_{i+1}|C_i]]\\
&=\mathbb E[\mathbb E[C_i(1+Z_i)|C_i]]\\
&=\mathbb E[C_i(1+\mathbb E[Z_i|C_i])]\\
&=\mathbb E[C_i(1+1/C_i)]\\
&=1+\mathbb E[C_i]
\end{align}$$
We have shown that, for each $i$, $\mathbb E[C_{i+1}]=1+\mathbb E[C_i]$.
Since $C_0=1$, we have:
$$\mathbb E[C_n]=1+\mathbb E[C_{n-1}]=n+1$$
Thus:
$$\mathbb E[C_n-1]=n$$
[...]
slide 23
- - -